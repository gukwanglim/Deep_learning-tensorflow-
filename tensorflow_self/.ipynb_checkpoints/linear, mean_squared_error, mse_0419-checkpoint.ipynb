{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4029c5d",
   "metadata": {},
   "source": [
    "# 15장 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c507bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e56e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd233d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "\n",
       "       11    12    13  \n",
       "0  396.90  4.98  24.0  \n",
       "1  396.90  9.14  21.6  \n",
       "2  392.83  4.03  34.7  \n",
       "3  394.63  2.94  33.4  \n",
       "4  396.90  5.33  36.2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../교재 코드/deeplearning-for-everyone-2nd-master/dataset/housing.csv',\n",
    "                  delim_whitespace = True,                                    # csv 파일이 ,가 아닌 ' '로 분리되어 있다.\n",
    "                 header = None)\n",
    "\n",
    "# df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b805f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       506 non-null    float64\n",
      " 1   1       506 non-null    float64\n",
      " 2   2       506 non-null    float64\n",
      " 3   3       506 non-null    int64  \n",
      " 4   4       506 non-null    float64\n",
      " 5   5       506 non-null    float64\n",
      " 6   6       506 non-null    float64\n",
      " 7   7       506 non-null    float64\n",
      " 8   8       506 non-null    int64  \n",
      " 9   9       506 non-null    float64\n",
      " 10  10      506 non-null    float64\n",
      " 11  11      506 non-null    float64\n",
      " 12  12      506 non-null    float64\n",
      " 13  13      506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd64b9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "x = df.values[:, :-1]\n",
    "y = df.values[:, -1]\n",
    "\n",
    "print(x.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba4ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b5f45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,\n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f932aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5df19b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 30)                420       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 186       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 613\n",
      "Trainable params: 613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(30, input_dim = 13, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(6, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'linear'))                       # 선형 회귀는 linear 사용\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba294fc",
   "metadata": {},
   "source": [
    "- 회귀의 경우, 마지막 Dense에서의 activation은 sigmoid가 아닌 linear를 사용한다.\n",
    "    - linear는 $w_1z_1 + w_2z_2 + ... + w_nz_n$의 식을 sigmoid를 사용하지 않고 직접 사용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1bf8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',                    # 회귀의 문제일 때는 mean_squared_error를 사용\n",
    "             optimizer = 'adam',\n",
    "             metrics = 'mse')                                # linear는 accuracy가 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d69fee",
   "metadata": {},
   "source": [
    "- 회귀의 문제일 경우 binary_crossentropy가 아닌 mean_squared_error loss를 사용한다.\n",
    "- 또한, accuracy가 없기 때문에 (선형 회귀이므로 정확도를 나타낼 수 없다.) mse를 사용한다. -> linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d46d2bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 255.6734 - mse: 255.6734WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 426.8181 - mse: 426.8181 - val_loss: 251.6882 - val_mse: 251.6882\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 132.6847 - mse: 132.6847 - val_loss: 117.8056 - val_mse: 117.8056\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 88.4084 - mse: 88.4084 - val_loss: 113.9318 - val_mse: 113.9318\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 85.5072 - mse: 85.5072 - val_loss: 98.1600 - val_mse: 98.1600\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 83.1752 - mse: 83.1752 - val_loss: 94.1898 - val_mse: 94.1898\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 76.8787 - mse: 76.8787 - val_loss: 91.2665 - val_mse: 91.2665\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 73.4317 - mse: 73.4317 - val_loss: 95.4517 - val_mse: 95.4517\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 70.8960 - mse: 70.8960 - val_loss: 93.7726 - val_mse: 93.7726\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 69.7683 - mse: 69.7683 - val_loss: 84.8152 - val_mse: 84.8152\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 65.8711 - mse: 65.8711 - val_loss: 83.5815 - val_mse: 83.5815\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 65.6352 - mse: 65.6352 - val_loss: 81.1606 - val_mse: 81.1606\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 66.6186 - mse: 66.6186 - val_loss: 78.9400 - val_mse: 78.9400\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 59.8231 - mse: 59.8231 - val_loss: 76.1864 - val_mse: 76.1864\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 57.6524 - mse: 57.6524 - val_loss: 75.2067 - val_mse: 75.2067\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 56.3149 - mse: 56.3149 - val_loss: 75.9611 - val_mse: 75.9611\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 58.3771 - mse: 58.3771 - val_loss: 71.2385 - val_mse: 71.2385\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 53.3945 - mse: 53.3945 - val_loss: 66.2445 - val_mse: 66.2445\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 53.7586 - mse: 53.7586 - val_loss: 71.2328 - val_mse: 71.2328\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 53.2579 - mse: 53.2579 - val_loss: 63.6562 - val_mse: 63.6562\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 50.5841 - mse: 50.5841 - val_loss: 61.8715 - val_mse: 61.8715\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 47.7209 - mse: 47.7209 - val_loss: 60.5302 - val_mse: 60.5302\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 47.1642 - mse: 47.1642 - val_loss: 59.8205 - val_mse: 59.8205\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 45.6080 - mse: 45.6080 - val_loss: 57.0778 - val_mse: 57.0778\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 43.5834 - mse: 43.5834 - val_loss: 55.6963 - val_mse: 55.6963\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 41.7553 - mse: 41.7553 - val_loss: 55.8748 - val_mse: 55.8748\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 42.4927 - mse: 42.4927 - val_loss: 53.1184 - val_mse: 53.1184\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 44.0686 - mse: 44.0686 - val_loss: 51.2534 - val_mse: 51.2534\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 40.2832 - mse: 40.2832 - val_loss: 52.1894 - val_mse: 52.1894\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 39.1303 - mse: 39.1303 - val_loss: 52.4572 - val_mse: 52.4572\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 40.3880 - mse: 40.3880 - val_loss: 51.1324 - val_mse: 51.1324\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 38.0006 - mse: 38.0006 - val_loss: 50.9483 - val_mse: 50.9483\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 37.9829 - mse: 37.9829 - val_loss: 52.0869 - val_mse: 52.0869\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 37.8810 - mse: 37.8810 - val_loss: 46.7481 - val_mse: 46.7481\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 37.2728 - mse: 37.2728 - val_loss: 50.8538 - val_mse: 50.8538\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 39.0061 - mse: 39.0061 - val_loss: 45.9111 - val_mse: 45.9111\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 37.8070 - mse: 37.8070 - val_loss: 50.2936 - val_mse: 50.2936\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 39.1196 - mse: 39.1196 - val_loss: 48.8389 - val_mse: 48.8389\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 35.1157 - mse: 35.1157 - val_loss: 44.9118 - val_mse: 44.9118\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 36.5383 - mse: 36.5383 - val_loss: 44.2228 - val_mse: 44.2228\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 34.4814 - mse: 34.4814 - val_loss: 45.5017 - val_mse: 45.5017\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 971us/step - loss: 34.5120 - mse: 34.5120 - val_loss: 48.6748 - val_mse: 48.6748\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 34.5009 - mse: 34.5009 - val_loss: 48.7893 - val_mse: 48.7893\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 35.9488 - mse: 35.9488 - val_loss: 42.4050 - val_mse: 42.4050\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 33.7133 - mse: 33.7133 - val_loss: 42.8856 - val_mse: 42.8856\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 33.9519 - mse: 33.9519 - val_loss: 41.2107 - val_mse: 41.2107\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 32.6812 - mse: 32.6812 - val_loss: 47.5133 - val_mse: 47.5133\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 32.6456 - mse: 32.6456 - val_loss: 44.5888 - val_mse: 44.5888\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 32.2968 - mse: 32.2968 - val_loss: 41.5106 - val_mse: 41.5106\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 31.3427 - mse: 31.3427 - val_loss: 42.9446 - val_mse: 42.9446\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 32.1260 - mse: 32.1260 - val_loss: 44.2105 - val_mse: 44.2105\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 33.8096 - mse: 33.8096 - val_loss: 44.4292 - val_mse: 44.4292\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 34.4567 - mse: 34.4567 - val_loss: 48.9865 - val_mse: 48.9865\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 30.4035 - mse: 30.4035 - val_loss: 43.8237 - val_mse: 43.8237\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 31.1799 - mse: 31.1799 - val_loss: 40.7319 - val_mse: 40.7319\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 30.6171 - mse: 30.6171 - val_loss: 39.3113 - val_mse: 39.3113\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 30.0328 - mse: 30.0328 - val_loss: 41.2386 - val_mse: 41.2386\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 31.3579 - mse: 31.3579 - val_loss: 42.3666 - val_mse: 42.3666\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 30.9564 - mse: 30.9564 - val_loss: 46.1536 - val_mse: 46.1536\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 28.8021 - mse: 28.8021 - val_loss: 38.6367 - val_mse: 38.6367\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 29.5784 - mse: 29.5784 - val_loss: 38.3881 - val_mse: 38.3881\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 29.1751 - mse: 29.1751 - val_loss: 38.3300 - val_mse: 38.3300\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 29.4822 - mse: 29.4822 - val_loss: 38.3101 - val_mse: 38.3101\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 36.5313 - mse: 36.5313 - val_loss: 40.0526 - val_mse: 40.0526\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 30.6341 - mse: 30.6341 - val_loss: 37.5203 - val_mse: 37.5203\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 30.7019 - mse: 30.7019 - val_loss: 40.0514 - val_mse: 40.0514\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.7946 - mse: 27.7946 - val_loss: 38.6321 - val_mse: 38.6321\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 908us/step - loss: 30.3273 - mse: 30.3273 - val_loss: 44.0991 - val_mse: 44.0991\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 33.3015 - mse: 33.3015 - val_loss: 39.9543 - val_mse: 39.9543\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 29.9062 - mse: 29.9062 - val_loss: 44.5621 - val_mse: 44.5621\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 28.8305 - mse: 28.8305 - val_loss: 38.9175 - val_mse: 38.9175\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 28.9340 - mse: 28.9340 - val_loss: 41.6461 - val_mse: 41.6461\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 29.3988 - mse: 29.3988 - val_loss: 39.1938 - val_mse: 39.1938\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 27.8452 - mse: 27.8452 - val_loss: 39.1489 - val_mse: 39.1489\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 28.6438 - mse: 28.6438 - val_loss: 39.3411 - val_mse: 39.3411\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 28.1412 - mse: 28.1412 - val_loss: 36.9223 - val_mse: 36.9223\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 948us/step - loss: 28.6304 - mse: 28.6304 - val_loss: 42.6596 - val_mse: 42.6596\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 33.2086 - mse: 33.2086 - val_loss: 36.1107 - val_mse: 36.1107\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 885us/step - loss: 32.0717 - mse: 32.0717 - val_loss: 45.1071 - val_mse: 45.1071\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 32.0719 - mse: 32.0719 - val_loss: 44.8675 - val_mse: 44.8675\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 33.6258 - mse: 33.6258 - val_loss: 41.6365 - val_mse: 41.6365\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.4620 - mse: 27.4620 - val_loss: 37.0935 - val_mse: 37.0935\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 27.6681 - mse: 27.6681 - val_loss: 38.8523 - val_mse: 38.8523\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 28.3303 - mse: 28.3303 - val_loss: 38.1596 - val_mse: 38.1596\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 27.3098 - mse: 27.3098 - val_loss: 37.5372 - val_mse: 37.5372\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 36.2336 - mse: 36.2336 - val_loss: 36.8008 - val_mse: 36.8008\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 31.8284 - mse: 31.8284 - val_loss: 35.4009 - val_mse: 35.4009\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 886us/step - loss: 27.4848 - mse: 27.4848 - val_loss: 40.5504 - val_mse: 40.5504\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.7958 - mse: 27.7958 - val_loss: 41.5481 - val_mse: 41.5481\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.6520 - mse: 27.6520 - val_loss: 38.3093 - val_mse: 38.3093\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.6974 - mse: 26.6974 - val_loss: 35.5939 - val_mse: 35.5939\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 25.7305 - mse: 25.7305 - val_loss: 41.8411 - val_mse: 41.8411\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 26.9909 - mse: 26.9909 - val_loss: 35.9729 - val_mse: 35.9729\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 25.6289 - mse: 25.6289 - val_loss: 36.0353 - val_mse: 36.0353\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 906us/step - loss: 26.2371 - mse: 26.2371 - val_loss: 35.7740 - val_mse: 35.7740\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 29.1261 - mse: 29.1261 - val_loss: 36.6007 - val_mse: 36.6007\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.3179 - mse: 26.3179 - val_loss: 35.3552 - val_mse: 35.3552\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.8397 - mse: 25.8397 - val_loss: 35.5659 - val_mse: 35.5659\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 26.9900 - mse: 26.9900 - val_loss: 35.5125 - val_mse: 35.5125\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 25.4180 - mse: 25.4180 - val_loss: 34.5008 - val_mse: 34.5008\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.4556 - mse: 27.4556 - val_loss: 35.4836 - val_mse: 35.4836\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.8918 - mse: 25.8918 - val_loss: 34.6808 - val_mse: 34.6808\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.1600 - mse: 27.1600 - val_loss: 38.7931 - val_mse: 38.7931\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.4680 - mse: 25.4680 - val_loss: 47.8896 - val_mse: 47.8896\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.2238 - mse: 26.2238 - val_loss: 34.2353 - val_mse: 34.2353\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 27.1465 - mse: 27.1465 - val_loss: 34.5075 - val_mse: 34.5075\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 28.2967 - mse: 28.2967 - val_loss: 36.2280 - val_mse: 36.2280\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.9404 - mse: 24.9404 - val_loss: 35.5440 - val_mse: 35.5440\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.9463 - mse: 26.9463 - val_loss: 40.5978 - val_mse: 40.5978\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 27.3762 - mse: 27.3762 - val_loss: 38.6353 - val_mse: 38.6353\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.8769 - mse: 27.8769 - val_loss: 35.7252 - val_mse: 35.7252\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.4404 - mse: 26.4404 - val_loss: 35.7730 - val_mse: 35.7730\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.5966 - mse: 24.5966 - val_loss: 36.0239 - val_mse: 36.0239\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 24.4989 - mse: 24.4989 - val_loss: 35.2216 - val_mse: 35.2216\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 24.6444 - mse: 24.6444 - val_loss: 34.8371 - val_mse: 34.8371\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.4968 - mse: 24.4968 - val_loss: 36.5857 - val_mse: 36.5857\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.4550 - mse: 26.4550 - val_loss: 34.6570 - val_mse: 34.6570\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 890us/step - loss: 25.4554 - mse: 25.4554 - val_loss: 38.8199 - val_mse: 38.8199\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.3105 - mse: 25.3105 - val_loss: 33.3829 - val_mse: 33.3829\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.5564 - mse: 25.5564 - val_loss: 34.2059 - val_mse: 34.2059\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.7773 - mse: 26.7773 - val_loss: 39.5911 - val_mse: 39.5911\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.3907 - mse: 25.3907 - val_loss: 33.5315 - val_mse: 33.5315\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.6364 - mse: 23.6364 - val_loss: 36.9299 - val_mse: 36.9299\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.6915 - mse: 25.6915 - val_loss: 39.8097 - val_mse: 39.8097\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.8569 - mse: 26.8569 - val_loss: 34.1817 - val_mse: 34.1817\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 24.7627 - mse: 24.7627 - val_loss: 34.6652 - val_mse: 34.6652\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.7030 - mse: 24.7030 - val_loss: 48.8078 - val_mse: 48.8078\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.9172 - mse: 27.9172 - val_loss: 36.8957 - val_mse: 36.8957\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.6094 - mse: 26.6094 - val_loss: 33.7142 - val_mse: 33.7142\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.1288 - mse: 27.1288 - val_loss: 34.8308 - val_mse: 34.8308\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.2308 - mse: 26.2308 - val_loss: 33.2841 - val_mse: 33.2841\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.9201 - mse: 25.9201 - val_loss: 35.5263 - val_mse: 35.5263\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.4653 - mse: 24.4653 - val_loss: 37.1642 - val_mse: 37.1642\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.1795 - mse: 25.1795 - val_loss: 35.3875 - val_mse: 35.3875\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 27.3331 - mse: 27.3331 - val_loss: 33.4128 - val_mse: 33.4128\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.5359 - mse: 24.5359 - val_loss: 35.4378 - val_mse: 35.4378\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.7826 - mse: 23.7826 - val_loss: 33.6002 - val_mse: 33.6002\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.5068 - mse: 24.5068 - val_loss: 33.2323 - val_mse: 33.2323\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 23.8103 - mse: 23.8103 - val_loss: 33.0486 - val_mse: 33.0486\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 24.1658 - mse: 24.1658 - val_loss: 34.7024 - val_mse: 34.7024\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.4688 - mse: 24.4688 - val_loss: 33.8386 - val_mse: 33.8386\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.7267 - mse: 24.7267 - val_loss: 35.8920 - val_mse: 35.8920\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 27.3654 - mse: 27.3654 - val_loss: 32.6088 - val_mse: 32.6088\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.7328 - mse: 23.7328 - val_loss: 31.7296 - val_mse: 31.7296\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 24.7958 - mse: 24.7958 - val_loss: 36.9266 - val_mse: 36.9266\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.6682 - mse: 23.6682 - val_loss: 32.1616 - val_mse: 32.1616\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.5688 - mse: 25.5688 - val_loss: 34.3416 - val_mse: 34.3416\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.3868 - mse: 26.3868 - val_loss: 34.0291 - val_mse: 34.0291\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 23.5830 - mse: 23.5830 - val_loss: 32.9845 - val_mse: 32.9845\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.1768 - mse: 26.1768 - val_loss: 33.8134 - val_mse: 33.8134\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.9759 - mse: 24.9759 - val_loss: 36.6045 - val_mse: 36.6045\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.4163 - mse: 26.4163 - val_loss: 34.9768 - val_mse: 34.9768\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 908us/step - loss: 23.6099 - mse: 23.6099 - val_loss: 34.6438 - val_mse: 34.6438\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.5487 - mse: 23.5487 - val_loss: 34.3623 - val_mse: 34.3623\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.1051 - mse: 23.1051 - val_loss: 32.9700 - val_mse: 32.9700\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.8349 - mse: 22.8349 - val_loss: 33.5786 - val_mse: 33.5786\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 23.9251 - mse: 23.9251 - val_loss: 34.6446 - val_mse: 34.6446\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.8159 - mse: 25.8159 - val_loss: 33.3757 - val_mse: 33.3757\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 26.6433 - mse: 26.6433 - val_loss: 33.4106 - val_mse: 33.4106\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.5217 - mse: 25.5217 - val_loss: 33.8760 - val_mse: 33.8760\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 24.8582 - mse: 24.8582 - val_loss: 36.1407 - val_mse: 36.1407\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 24.8201 - mse: 24.8201 - val_loss: 33.5997 - val_mse: 33.5997\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 912us/step - loss: 24.6895 - mse: 24.6895 - val_loss: 34.6958 - val_mse: 34.6958\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 24.4931 - mse: 24.4931 - val_loss: 31.8664 - val_mse: 31.8664\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.9887 - mse: 22.9887 - val_loss: 32.2909 - val_mse: 32.2909\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 24.0494 - mse: 24.0494 - val_loss: 32.9132 - val_mse: 32.9132\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 23.6997 - mse: 23.6997 - val_loss: 32.3881 - val_mse: 32.3881\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 24.2496 - mse: 24.2496 - val_loss: 30.9988 - val_mse: 30.9988\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.0715 - mse: 22.0715 - val_loss: 33.7449 - val_mse: 33.7449\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.3263 - mse: 23.3263 - val_loss: 32.4471 - val_mse: 32.4471\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.3412 - mse: 24.3412 - val_loss: 31.8886 - val_mse: 31.8886\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 26.0169 - mse: 26.0169 - val_loss: 30.9408 - val_mse: 30.9408\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.3962 - mse: 24.3962 - val_loss: 31.0850 - val_mse: 31.0850\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.4904 - mse: 25.4904 - val_loss: 31.3135 - val_mse: 31.3135\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.6751 - mse: 27.6751 - val_loss: 52.5597 - val_mse: 52.5597\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 925us/step - loss: 24.3465 - mse: 24.3465 - val_loss: 34.5464 - val_mse: 34.5464\n",
      "Epoch 176/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 918us/step - loss: 25.9968 - mse: 25.9968 - val_loss: 35.4569 - val_mse: 35.4569\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 856us/step - loss: 23.5399 - mse: 23.5399 - val_loss: 32.3217 - val_mse: 32.3217\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 22.9483 - mse: 22.9483 - val_loss: 34.6801 - val_mse: 34.6801\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 30.2020 - mse: 30.2020 - val_loss: 34.6991 - val_mse: 34.6991\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.0910 - mse: 23.0910 - val_loss: 32.9224 - val_mse: 32.9224\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 22.4509 - mse: 22.4509 - val_loss: 32.5229 - val_mse: 32.5229\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 24.0631 - mse: 24.0631 - val_loss: 30.8786 - val_mse: 30.8786\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.4237 - mse: 22.4237 - val_loss: 35.0304 - val_mse: 35.0304\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 27.2007 - mse: 27.2007 - val_loss: 34.6623 - val_mse: 34.6623\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.4756 - mse: 23.4756 - val_loss: 31.7240 - val_mse: 31.7240\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.1298 - mse: 23.1298 - val_loss: 32.9703 - val_mse: 32.9703\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.3121 - mse: 22.3121 - val_loss: 35.2351 - val_mse: 35.2351\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.8115 - mse: 22.8115 - val_loss: 31.9469 - val_mse: 31.9469\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.0276 - mse: 22.0276 - val_loss: 31.3360 - val_mse: 31.3360\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.7094 - mse: 22.7094 - val_loss: 32.1811 - val_mse: 32.1811\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.5568 - mse: 21.5568 - val_loss: 31.5865 - val_mse: 31.5865\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.1509 - mse: 22.1509 - val_loss: 30.7422 - val_mse: 30.7422\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.1723 - mse: 22.1723 - val_loss: 34.4290 - val_mse: 34.4290\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.6143 - mse: 22.6143 - val_loss: 33.5197 - val_mse: 33.5197\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.3584 - mse: 22.3584 - val_loss: 31.5313 - val_mse: 31.5313\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.0264 - mse: 22.0264 - val_loss: 31.8728 - val_mse: 31.8728\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.8057 - mse: 23.8057 - val_loss: 43.6550 - val_mse: 43.6550\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 26.7676 - mse: 26.7676 - val_loss: 34.8933 - val_mse: 34.8933\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.7759 - mse: 22.7759 - val_loss: 32.1118 - val_mse: 32.1118\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.5622 - mse: 22.5622 - val_loss: 34.1525 - val_mse: 34.1525\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.4360 - mse: 22.4360 - val_loss: 41.5217 - val_mse: 41.5217\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 23.0651 - mse: 23.0651 - val_loss: 30.5023 - val_mse: 30.5023\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.2842 - mse: 26.2842 - val_loss: 33.4128 - val_mse: 33.4128\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 26.0129 - mse: 26.0129 - val_loss: 32.7149 - val_mse: 32.7149\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.3458 - mse: 22.3458 - val_loss: 32.4649 - val_mse: 32.4649\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 22.6383 - mse: 22.6383 - val_loss: 30.5546 - val_mse: 30.5546\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 23.2056 - mse: 23.2056 - val_loss: 31.1289 - val_mse: 31.1289\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.8468 - mse: 22.8467 - val_loss: 32.3635 - val_mse: 32.3635\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 23.0935 - mse: 23.0935 - val_loss: 30.9589 - val_mse: 30.9589\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.4651 - mse: 22.4651 - val_loss: 30.1514 - val_mse: 30.1514\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.7375 - mse: 21.7375 - val_loss: 31.8227 - val_mse: 31.8227\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.5124 - mse: 21.5124 - val_loss: 32.3490 - val_mse: 32.3490\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 21.5504 - mse: 21.5504 - val_loss: 30.2477 - val_mse: 30.2477\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.2097 - mse: 22.2097 - val_loss: 34.5652 - val_mse: 34.5652\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 22.5681 - mse: 22.5681 - val_loss: 31.9982 - val_mse: 31.9982\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.3338 - mse: 24.3338 - val_loss: 34.2361 - val_mse: 34.2361\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 20.7243 - mse: 20.7243 - val_loss: 31.4361 - val_mse: 31.4361\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.6104 - mse: 22.6104 - val_loss: 32.4484 - val_mse: 32.4484\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 21.3715 - mse: 21.3715 - val_loss: 31.1816 - val_mse: 31.1816\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.0423 - mse: 23.0423 - val_loss: 32.0683 - val_mse: 32.0683\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 21.5579 - mse: 21.5579 - val_loss: 32.5341 - val_mse: 32.5341\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.1150 - mse: 22.1150 - val_loss: 33.9503 - val_mse: 33.9503\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 21.5054 - mse: 21.5054 - val_loss: 35.6383 - val_mse: 35.6382\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 21.0148 - mse: 21.0148 - val_loss: 33.9961 - val_mse: 33.9961\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 21.6081 - mse: 21.6081 - val_loss: 31.4046 - val_mse: 31.4046\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 21.7657 - mse: 21.7657 - val_loss: 34.2004 - val_mse: 34.2004\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.1188 - mse: 22.1188 - val_loss: 31.4963 - val_mse: 31.4963\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 25.3636 - mse: 25.3636 - val_loss: 32.3775 - val_mse: 32.3775\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.6814 - mse: 22.6814 - val_loss: 31.0039 - val_mse: 31.0039\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 927us/step - loss: 22.3797 - mse: 22.3797 - val_loss: 30.9498 - val_mse: 30.9498\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 21.7324 - mse: 21.7324 - val_loss: 37.3878 - val_mse: 37.3878\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.4106 - mse: 24.4106 - val_loss: 31.2247 - val_mse: 31.2247\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 21.3799 - mse: 21.3799 - val_loss: 37.5053 - val_mse: 37.5053\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.2768 - mse: 22.2768 - val_loss: 30.2289 - val_mse: 30.2289\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 878us/step - loss: 21.3826 - mse: 21.3826 - val_loss: 31.0096 - val_mse: 31.0096\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.5750 - mse: 20.5750 - val_loss: 31.5599 - val_mse: 31.5599\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 23.3559 - mse: 23.3559 - val_loss: 31.6757 - val_mse: 31.6757\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 22.3814 - mse: 22.3814 - val_loss: 30.0562 - val_mse: 30.0562\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 21.3230 - mse: 21.3230 - val_loss: 30.9265 - val_mse: 30.9265\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 21.7025 - mse: 21.7025 - val_loss: 31.8233 - val_mse: 31.8233\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.6275 - mse: 20.6275 - val_loss: 30.7237 - val_mse: 30.7237\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 23.1338 - mse: 23.1338 - val_loss: 30.1062 - val_mse: 30.1062\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 23.4966 - mse: 23.4966 - val_loss: 31.2365 - val_mse: 31.2365\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 22.9143 - mse: 22.9143 - val_loss: 37.2288 - val_mse: 37.2288\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 24.2149 - mse: 24.2149 - val_loss: 32.0956 - val_mse: 32.0956\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 23.1777 - mse: 23.1777 - val_loss: 30.1228 - val_mse: 30.1228\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.3989 - mse: 20.3989 - val_loss: 30.4141 - val_mse: 30.4141\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.5546 - mse: 21.5546 - val_loss: 34.0403 - val_mse: 34.0403\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.1992 - mse: 23.1992 - val_loss: 32.0893 - val_mse: 32.0893\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.7902 - mse: 20.7902 - val_loss: 30.7706 - val_mse: 30.7706\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 20.6718 - mse: 20.6718 - val_loss: 32.5895 - val_mse: 32.5895\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.1144 - mse: 21.1144 - val_loss: 34.5235 - val_mse: 34.5235\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 23.8450 - mse: 23.8450 - val_loss: 29.8820 - val_mse: 29.8820\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 21.3700 - mse: 21.3700 - val_loss: 30.3265 - val_mse: 30.3265\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.3541 - mse: 22.3541 - val_loss: 32.9679 - val_mse: 32.9679\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 20.7842 - mse: 20.7842 - val_loss: 35.9694 - val_mse: 35.9694\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 21.5824 - mse: 21.5824 - val_loss: 30.7440 - val_mse: 30.7440\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.2589 - mse: 20.2589 - val_loss: 32.0411 - val_mse: 32.0411\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.1080 - mse: 20.1080 - val_loss: 29.9608 - val_mse: 29.9608\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.4609 - mse: 20.4609 - val_loss: 30.5369 - val_mse: 30.5369\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.3153 - mse: 22.3153 - val_loss: 34.6587 - val_mse: 34.6587\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 22.3553 - mse: 22.3553 - val_loss: 30.6360 - val_mse: 30.6360\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 21.0528 - mse: 21.0528 - val_loss: 34.4575 - val_mse: 34.4575\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 25.2768 - mse: 25.2768 - val_loss: 38.4557 - val_mse: 38.4557\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 21.9611 - mse: 21.9611 - val_loss: 28.9380 - val_mse: 28.9380\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 20.8541 - mse: 20.8541 - val_loss: 29.9816 - val_mse: 29.9816\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 19.9942 - mse: 19.9942 - val_loss: 31.7944 - val_mse: 31.7944\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.6319 - mse: 20.6319 - val_loss: 29.0009 - val_mse: 29.0009\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.4095 - mse: 19.4095 - val_loss: 32.4902 - val_mse: 32.4902\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 22.4494 - mse: 22.4494 - val_loss: 28.9592 - val_mse: 28.9592\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.1125 - mse: 20.1125 - val_loss: 30.2002 - val_mse: 30.2002\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 21.6537 - mse: 21.6537 - val_loss: 29.3905 - val_mse: 29.3905\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 893us/step - loss: 20.0084 - mse: 20.0084 - val_loss: 32.0590 - val_mse: 32.0590\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 21.4313 - mse: 21.4313 - val_loss: 29.5295 - val_mse: 29.5295\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 20.8334 - mse: 20.8334 - val_loss: 29.6720 - val_mse: 29.6720\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 20.0552 - mse: 20.0552 - val_loss: 29.4169 - val_mse: 29.4169\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.6783 - mse: 22.6783 - val_loss: 33.3512 - val_mse: 33.3512\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 882us/step - loss: 22.2404 - mse: 22.2404 - val_loss: 33.0093 - val_mse: 33.0093\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.9314 - mse: 19.9314 - val_loss: 29.7776 - val_mse: 29.7776\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 19.9770 - mse: 19.9770 - val_loss: 29.6100 - val_mse: 29.6100\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.6555 - mse: 22.6555 - val_loss: 29.8522 - val_mse: 29.8522\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.1419 - mse: 19.1419 - val_loss: 28.7636 - val_mse: 28.7636\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 20.3606 - mse: 20.3606 - val_loss: 28.3735 - val_mse: 28.3735\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.2615 - mse: 22.2615 - val_loss: 31.3306 - val_mse: 31.3306\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 867us/step - loss: 20.6541 - mse: 20.6541 - val_loss: 29.2612 - val_mse: 29.2612\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 20.3956 - mse: 20.3956 - val_loss: 29.0170 - val_mse: 29.0170\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 21.6109 - mse: 21.6109 - val_loss: 31.1768 - val_mse: 31.1768\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 22.5011 - mse: 22.5011 - val_loss: 29.2231 - val_mse: 29.2231\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 21.0113 - mse: 21.0113 - val_loss: 29.7590 - val_mse: 29.7590\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.0537 - mse: 19.0537 - val_loss: 31.3751 - val_mse: 31.3751\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.2921 - mse: 20.2921 - val_loss: 33.0884 - val_mse: 33.0884\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.5054 - mse: 20.5054 - val_loss: 28.3596 - val_mse: 28.3596\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 19.0021 - mse: 19.0021 - val_loss: 28.7209 - val_mse: 28.7209\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 878us/step - loss: 20.6165 - mse: 20.6165 - val_loss: 28.9005 - val_mse: 28.9005\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.5789 - mse: 21.5789 - val_loss: 28.9895 - val_mse: 28.9895\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.4014 - mse: 19.4014 - val_loss: 31.0614 - val_mse: 31.0614\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.2785 - mse: 20.2785 - val_loss: 29.0157 - val_mse: 29.0157\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 18.8451 - mse: 18.8451 - val_loss: 30.3052 - val_mse: 30.3052\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 19.0470 - mse: 19.0470 - val_loss: 29.7545 - val_mse: 29.7545\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 22.0822 - mse: 22.0822 - val_loss: 32.8046 - val_mse: 32.8046\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.3813 - mse: 19.3813 - val_loss: 29.6129 - val_mse: 29.6129\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.2434 - mse: 20.2434 - val_loss: 28.7917 - val_mse: 28.7917\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.7585 - mse: 19.7585 - val_loss: 30.3576 - val_mse: 30.3576\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.8429 - mse: 20.8429 - val_loss: 31.5177 - val_mse: 31.5177\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 19.5798 - mse: 19.5798 - val_loss: 31.2238 - val_mse: 31.2238\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 20.3355 - mse: 20.3355 - val_loss: 29.8000 - val_mse: 29.8000\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 19.8041 - mse: 19.8041 - val_loss: 29.7016 - val_mse: 29.7016\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 22.1405 - mse: 22.1405 - val_loss: 33.4022 - val_mse: 33.4022\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 21.4325 - mse: 21.4325 - val_loss: 34.0145 - val_mse: 34.0145\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.1545 - mse: 20.1545 - val_loss: 28.5855 - val_mse: 28.5855\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 18.0774 - mse: 18.0774 - val_loss: 28.9779 - val_mse: 28.9779\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 949us/step - loss: 20.1056 - mse: 20.1056 - val_loss: 28.1076 - val_mse: 28.1076\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.1223 - mse: 19.1223 - val_loss: 37.6892 - val_mse: 37.6892\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 18.9831 - mse: 18.9831 - val_loss: 29.7127 - val_mse: 29.7127\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 18.9317 - mse: 18.9317 - val_loss: 30.7382 - val_mse: 30.7382\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 19.1816 - mse: 19.1816 - val_loss: 29.7603 - val_mse: 29.7603\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 20.7703 - mse: 20.7703 - val_loss: 28.9646 - val_mse: 28.9646\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 19.7141 - mse: 19.7141 - val_loss: 28.8114 - val_mse: 28.8114\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.2383 - mse: 20.2383 - val_loss: 29.5865 - val_mse: 29.5865\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.5096 - mse: 19.5096 - val_loss: 28.3085 - val_mse: 28.3085\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 19.2654 - mse: 19.2654 - val_loss: 31.3588 - val_mse: 31.3588\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.8225 - mse: 20.8225 - val_loss: 28.7834 - val_mse: 28.7834\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 19.0714 - mse: 19.0714 - val_loss: 32.6397 - val_mse: 32.6397\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.4595 - mse: 19.4595 - val_loss: 30.6102 - val_mse: 30.6102\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.2721 - mse: 19.2721 - val_loss: 28.4760 - val_mse: 28.4760\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 881us/step - loss: 19.7715 - mse: 19.7715 - val_loss: 28.0185 - val_mse: 28.0185\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.7145 - mse: 18.7145 - val_loss: 29.2022 - val_mse: 29.2022\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.2209 - mse: 18.2209 - val_loss: 34.0467 - val_mse: 34.0467\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.9951 - mse: 18.9951 - val_loss: 30.5694 - val_mse: 30.5694\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.4237 - mse: 19.4237 - val_loss: 29.8184 - val_mse: 29.8184\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.5407 - mse: 18.5407 - val_loss: 29.6069 - val_mse: 29.6069\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.6001 - mse: 18.6001 - val_loss: 28.2899 - val_mse: 28.2899\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.8003 - mse: 16.8003 - val_loss: 37.2449 - val_mse: 37.2449\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 23.1368 - mse: 23.1368 - val_loss: 31.0729 - val_mse: 31.0729\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 17.9931 - mse: 17.9931 - val_loss: 30.2749 - val_mse: 30.2749\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.9144 - mse: 18.9144 - val_loss: 28.8362 - val_mse: 28.8362\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 869us/step - loss: 17.8781 - mse: 17.8781 - val_loss: 28.3699 - val_mse: 28.3699\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.5603 - mse: 17.5603 - val_loss: 29.0728 - val_mse: 29.0728\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.6494 - mse: 19.6494 - val_loss: 29.2471 - val_mse: 29.2471\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.4783 - mse: 17.4783 - val_loss: 31.5975 - val_mse: 31.5975\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.4435 - mse: 19.4435 - val_loss: 30.4405 - val_mse: 30.4405\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 18.7219 - mse: 18.7219 - val_loss: 27.7806 - val_mse: 27.7806\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.0216 - mse: 17.0216 - val_loss: 29.6929 - val_mse: 29.6929\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.9160 - mse: 18.9160 - val_loss: 27.6998 - val_mse: 27.6998\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 17.5125 - mse: 17.5125 - val_loss: 30.1397 - val_mse: 30.1397\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.2799 - mse: 18.2799 - val_loss: 28.8961 - val_mse: 28.8961\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.0449 - mse: 18.0449 - val_loss: 28.1202 - val_mse: 28.1202\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.6929 - mse: 18.6929 - val_loss: 35.4205 - val_mse: 35.4205\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.6991 - mse: 19.6991 - val_loss: 28.6703 - val_mse: 28.6703\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 17.7166 - mse: 17.7166 - val_loss: 27.8187 - val_mse: 27.8187\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.3109 - mse: 18.3109 - val_loss: 29.4973 - val_mse: 29.4973\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.6739 - mse: 16.6739 - val_loss: 31.9232 - val_mse: 31.9232\n",
      "Epoch 353/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 918us/step - loss: 17.8466 - mse: 17.8466 - val_loss: 32.9243 - val_mse: 32.9243\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 17.6818 - mse: 17.6818 - val_loss: 28.8856 - val_mse: 28.8856\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.5799 - mse: 17.5799 - val_loss: 27.5938 - val_mse: 27.5938\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.8904 - mse: 22.8904 - val_loss: 41.8084 - val_mse: 41.8084\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.6167 - mse: 18.6167 - val_loss: 28.0408 - val_mse: 28.0408\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.9809 - mse: 16.9809 - val_loss: 28.9417 - val_mse: 28.9417\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.2502 - mse: 17.2502 - val_loss: 29.3615 - val_mse: 29.3615\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 17.2440 - mse: 17.2440 - val_loss: 32.2295 - val_mse: 32.2295\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.1535 - mse: 19.1535 - val_loss: 27.4412 - val_mse: 27.4412\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.9551 - mse: 17.9551 - val_loss: 27.6418 - val_mse: 27.6418\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 942us/step - loss: 16.8531 - mse: 16.8531 - val_loss: 28.6232 - val_mse: 28.6232\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.5655 - mse: 18.5655 - val_loss: 31.8230 - val_mse: 31.8230\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 21.4411 - mse: 21.4411 - val_loss: 30.1925 - val_mse: 30.1925\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.1216 - mse: 17.1216 - val_loss: 28.0162 - val_mse: 28.0162\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.6652 - mse: 16.6652 - val_loss: 31.7769 - val_mse: 31.7769\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.0186 - mse: 18.0186 - val_loss: 31.1010 - val_mse: 31.1010\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 18.7420 - mse: 18.7420 - val_loss: 29.0205 - val_mse: 29.0205\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.2915 - mse: 18.2915 - val_loss: 28.7055 - val_mse: 28.7055\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.8307 - mse: 17.8307 - val_loss: 27.9451 - val_mse: 27.9451\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 20.9252 - mse: 20.9252 - val_loss: 30.0954 - val_mse: 30.0954\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 20.8011 - mse: 20.8011 - val_loss: 34.1757 - val_mse: 34.1757\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 18.3609 - mse: 18.3609 - val_loss: 32.4482 - val_mse: 32.4482\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 18.5804 - mse: 18.5804 - val_loss: 28.4125 - val_mse: 28.4125\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.6151 - mse: 18.6151 - val_loss: 26.8748 - val_mse: 26.8748\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 17.0302 - mse: 17.0302 - val_loss: 29.0334 - val_mse: 29.0334\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 16.6481 - mse: 16.6481 - val_loss: 29.2016 - val_mse: 29.2016\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.3475 - mse: 16.3475 - val_loss: 28.1215 - val_mse: 28.1215\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 18.1391 - mse: 18.1391 - val_loss: 29.0824 - val_mse: 29.0824\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 16.5915 - mse: 16.5915 - val_loss: 27.3641 - val_mse: 27.3641\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.2957 - mse: 17.2957 - val_loss: 31.9181 - val_mse: 31.9181\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.2917 - mse: 16.2917 - val_loss: 28.4897 - val_mse: 28.4897\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.0777 - mse: 17.0777 - val_loss: 30.5317 - val_mse: 30.5317\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 16.7085 - mse: 16.7085 - val_loss: 27.7214 - val_mse: 27.7214\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.5866 - mse: 16.5866 - val_loss: 26.7805 - val_mse: 26.7805\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 16.0825 - mse: 16.0825 - val_loss: 27.3084 - val_mse: 27.3084\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.3723 - mse: 16.3723 - val_loss: 29.9356 - val_mse: 29.9356\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.6827 - mse: 16.6827 - val_loss: 29.1888 - val_mse: 29.1888\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.2816 - mse: 16.2816 - val_loss: 27.1513 - val_mse: 27.1513\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 16.0660 - mse: 16.0660 - val_loss: 30.1096 - val_mse: 30.1096\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 19.2242 - mse: 19.2242 - val_loss: 36.2464 - val_mse: 36.2464\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 17.3999 - mse: 17.3999 - val_loss: 30.7271 - val_mse: 30.7271\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 17.0844 - mse: 17.0844 - val_loss: 29.9580 - val_mse: 29.9580\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.2720 - mse: 16.2720 - val_loss: 26.8687 - val_mse: 26.8687\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 16.4261 - mse: 16.4261 - val_loss: 27.1636 - val_mse: 27.1636\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 16.1576 - mse: 16.1576 - val_loss: 28.3365 - val_mse: 28.3365\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 15.4995 - mse: 15.4995 - val_loss: 28.5007 - val_mse: 28.5007\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.5831 - mse: 16.5831 - val_loss: 34.2716 - val_mse: 34.2716\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.3587 - mse: 17.3587 - val_loss: 29.8674 - val_mse: 29.8674\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.9739 - mse: 19.9739 - val_loss: 28.9784 - val_mse: 28.9784\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 17.7927 - mse: 17.7927 - val_loss: 27.5648 - val_mse: 27.5648\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 15.4395 - mse: 15.4395 - val_loss: 29.7010 - val_mse: 29.7010\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 19.8260 - mse: 19.8260 - val_loss: 41.5426 - val_mse: 41.5426\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.4196 - mse: 16.4196 - val_loss: 27.7431 - val_mse: 27.7431\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 16.3356 - mse: 16.3356 - val_loss: 27.0014 - val_mse: 27.0014\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.0791 - mse: 15.0791 - val_loss: 27.4849 - val_mse: 27.4849\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.3568 - mse: 15.3568 - val_loss: 31.2992 - val_mse: 31.2992\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.9100 - mse: 16.9100 - val_loss: 29.7666 - val_mse: 29.7666\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.6416 - mse: 17.6416 - val_loss: 38.5744 - val_mse: 38.5744\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.8725 - mse: 16.8725 - val_loss: 29.7332 - val_mse: 29.7332\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.3140 - mse: 17.3140 - val_loss: 31.0086 - val_mse: 31.0086\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.6317 - mse: 17.6317 - val_loss: 29.7137 - val_mse: 29.7137\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.4309 - mse: 17.4309 - val_loss: 27.6510 - val_mse: 27.6510\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 15.3952 - mse: 15.3952 - val_loss: 26.3898 - val_mse: 26.3898\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 15.4966 - mse: 15.4966 - val_loss: 25.9758 - val_mse: 25.9758\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.6699 - mse: 15.6699 - val_loss: 26.5762 - val_mse: 26.5762\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.4600 - mse: 15.4600 - val_loss: 26.9030 - val_mse: 26.9030\n",
      "Epoch 419/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 17.7487 - mse: 17.7487 - val_loss: 25.6536 - val_mse: 25.6536\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.6455 - mse: 17.6455 - val_loss: 26.5384 - val_mse: 26.5384\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 15.8795 - mse: 15.8795 - val_loss: 26.0261 - val_mse: 26.0261\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.4672 - mse: 14.4672 - val_loss: 26.6795 - val_mse: 26.6795\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.3076 - mse: 16.3076 - val_loss: 32.5359 - val_mse: 32.5359\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 17.9102 - mse: 17.9102 - val_loss: 28.5748 - val_mse: 28.5748\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.7059 - mse: 17.7059 - val_loss: 26.6555 - val_mse: 26.6555\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.6228 - mse: 14.6228 - val_loss: 27.5551 - val_mse: 27.5551\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 16.3756 - mse: 16.3756 - val_loss: 29.0953 - val_mse: 29.0953\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 15.4321 - mse: 15.4321 - val_loss: 28.3111 - val_mse: 28.3111\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 15.1965 - mse: 15.1965 - val_loss: 31.3480 - val_mse: 31.3480\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.8159 - mse: 17.8159 - val_loss: 39.4697 - val_mse: 39.4697\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 22.6707 - mse: 22.6707 - val_loss: 26.6357 - val_mse: 26.6357\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 16.2542 - mse: 16.2542 - val_loss: 28.0389 - val_mse: 28.0389\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.7392 - mse: 14.7392 - val_loss: 27.0894 - val_mse: 27.0894\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 18.6170 - mse: 18.6170 - val_loss: 31.8434 - val_mse: 31.8434\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.9458 - mse: 15.9458 - val_loss: 29.4046 - val_mse: 29.4046\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 917us/step - loss: 14.6634 - mse: 14.6634 - val_loss: 26.6736 - val_mse: 26.6736\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.5829 - mse: 14.5829 - val_loss: 28.5152 - val_mse: 28.5152\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.6392 - mse: 14.6392 - val_loss: 28.9880 - val_mse: 28.9880\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 15.6857 - mse: 15.6857 - val_loss: 28.6890 - val_mse: 28.6890\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 16.3607 - mse: 16.3607 - val_loss: 26.8103 - val_mse: 26.8103\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.9596 - mse: 14.9596 - val_loss: 27.0855 - val_mse: 27.0855\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.6192 - mse: 14.6192 - val_loss: 26.3995 - val_mse: 26.3995\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 18.4832 - mse: 18.4832 - val_loss: 40.8280 - val_mse: 40.8280\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 20.4560 - mse: 20.4560 - val_loss: 28.0295 - val_mse: 28.0295\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 15.5437 - mse: 15.5437 - val_loss: 28.4036 - val_mse: 28.4036\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 14.9724 - mse: 14.9724 - val_loss: 27.5479 - val_mse: 27.5479\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.8730 - mse: 14.8730 - val_loss: 28.4091 - val_mse: 28.4091\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.2891 - mse: 15.2891 - val_loss: 26.2772 - val_mse: 26.2772\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 16.7672 - mse: 16.7672 - val_loss: 29.2279 - val_mse: 29.2279\n",
      "Epoch 450/500\n",
      "25/25 [==============================] - 0s 958us/step - loss: 16.1306 - mse: 16.1306 - val_loss: 30.4843 - val_mse: 30.4843\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 15.4658 - mse: 15.4658 - val_loss: 26.6612 - val_mse: 26.6612\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 16.1249 - mse: 16.1249 - val_loss: 26.2967 - val_mse: 26.2967\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 14.2604 - mse: 14.2604 - val_loss: 25.9334 - val_mse: 25.9334\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 14.5217 - mse: 14.5217 - val_loss: 31.6563 - val_mse: 31.6563\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.6981 - mse: 14.6981 - val_loss: 25.5319 - val_mse: 25.5319\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 14.4235 - mse: 14.4235 - val_loss: 28.2682 - val_mse: 28.2682\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 14.6996 - mse: 14.6996 - val_loss: 26.4760 - val_mse: 26.4760\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 14.2012 - mse: 14.2012 - val_loss: 26.9749 - val_mse: 26.9749\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.7340 - mse: 14.7340 - val_loss: 25.5832 - val_mse: 25.5832\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 15.2564 - mse: 15.2564 - val_loss: 27.5508 - val_mse: 27.5508\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 16.3240 - mse: 16.3240 - val_loss: 27.1248 - val_mse: 27.1248\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 15.0786 - mse: 15.0786 - val_loss: 33.9922 - val_mse: 33.9922\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.5575 - mse: 15.5575 - val_loss: 27.2381 - val_mse: 27.2381\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 14.8003 - mse: 14.8003 - val_loss: 27.1135 - val_mse: 27.1135\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 15.5528 - mse: 15.5528 - val_loss: 27.7182 - val_mse: 27.7182\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 16.6230 - mse: 16.6230 - val_loss: 26.6039 - val_mse: 26.6039\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.3415 - mse: 14.3415 - val_loss: 27.0667 - val_mse: 27.0667\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.0655 - mse: 14.0655 - val_loss: 25.5600 - val_mse: 25.5600\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 14.4398 - mse: 14.4398 - val_loss: 26.4314 - val_mse: 26.4314\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.5650 - mse: 14.5650 - val_loss: 31.5945 - val_mse: 31.5945\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 878us/step - loss: 13.6136 - mse: 13.6136 - val_loss: 27.1603 - val_mse: 27.1603\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 16.4065 - mse: 16.4065 - val_loss: 25.2093 - val_mse: 25.2093\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.0786 - mse: 15.0786 - val_loss: 26.3685 - val_mse: 26.3685\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 13.9491 - mse: 13.9491 - val_loss: 27.3435 - val_mse: 27.3435\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 15.6396 - mse: 15.6396 - val_loss: 26.6835 - val_mse: 26.6835\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 14.6478 - mse: 14.6478 - val_loss: 26.6948 - val_mse: 26.6948\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.7389 - mse: 14.7389 - val_loss: 26.1532 - val_mse: 26.1532\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.0828 - mse: 17.0828 - val_loss: 33.8077 - val_mse: 33.8077\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 15.7437 - mse: 15.7437 - val_loss: 27.9339 - val_mse: 27.9339\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 14.8531 - mse: 14.8531 - val_loss: 25.3275 - val_mse: 25.3275\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 13.9395 - mse: 13.9395 - val_loss: 26.5343 - val_mse: 26.5343\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 13.6962 - mse: 13.6962 - val_loss: 28.6250 - val_mse: 28.6250\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.7843 - mse: 14.7843 - val_loss: 25.5668 - val_mse: 25.5668\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 14.4685 - mse: 14.4685 - val_loss: 24.3018 - val_mse: 24.3018\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 13.5855 - mse: 13.5855 - val_loss: 24.4967 - val_mse: 24.4967\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 13.9972 - mse: 13.9972 - val_loss: 27.9897 - val_mse: 27.9897\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 17.8534 - mse: 17.8534 - val_loss: 26.4133 - val_mse: 26.4133\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 16.8987 - mse: 16.8987 - val_loss: 30.5690 - val_mse: 30.5690\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 14.3964 - mse: 14.3964 - val_loss: 25.3711 - val_mse: 25.3711\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.2975 - mse: 14.2975 - val_loss: 29.2432 - val_mse: 29.2432\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 17.1047 - mse: 17.1047 - val_loss: 31.2913 - val_mse: 31.2913\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 16.4758 - mse: 16.4758 - val_loss: 25.9341 - val_mse: 25.9341\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 15.0274 - mse: 15.0274 - val_loss: 29.1601 - val_mse: 29.1601\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 14.4094 - mse: 14.4094 - val_loss: 22.5506 - val_mse: 22.5506\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 13.7871 - mse: 13.7871 - val_loss: 28.1753 - val_mse: 28.1753\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 16.8400 - mse: 16.8400 - val_loss: 24.7862 - val_mse: 24.7862\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 838us/step - loss: 14.3411 - mse: 14.3411 - val_loss: 27.6577 - val_mse: 27.6577\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 878us/step - loss: 13.9310 - mse: 13.9310 - val_loss: 24.9949 - val_mse: 24.9949\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 14.5016 - mse: 14.5016 - val_loss: 25.4934 - val_mse: 25.4934\n",
      "Epoch 500/500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 14.1259 - mse: 14.1259 - val_loss: 26.5988 - val_mse: 26.5988\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 500, batch_size = 10, \n",
    "                   validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "579921c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e181f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], c = 'r')\n",
    "plt.plot(history.history['val_loss'], c = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c461f",
   "metadata": {},
   "source": [
    "- 회귀의 경우, 마지막 Dense에서의 activation은 sigmoid가 아닌 linear를 사용한다.\n",
    "    - linear는 $w_1z_1 + w_2z_2 + ... + w_nz_n$의 식을 sigmoid를 사용하지 않고 직접 사용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5becf2",
   "metadata": {},
   "source": [
    "- 회귀의 문제일 경우 binary_crossentropy가 아닌 mean_squared_error loss를 사용한다.\n",
    "- 또한, accuracy가 없기 때문에 (선형 회귀이므로 정확도를 나타낼 수 없다.) mse를 사용한다. -> linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee65389",
   "metadata": {},
   "source": [
    "- 회귀의 문제일 경우, model.fit(x_train, y_train)을 넣어서 사용."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
