{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db52ef6",
   "metadata": {},
   "source": [
    "# 오류 역전파(Back Propagation)\n",
    "\n",
    "<span style = 'color:red'>가중치를 몰라도 문제를 해결</span>하기 위해 개발된 방법.\n",
    "\n",
    "분류 중 에러가 발생하면 에러가 걸린 레이어를 뒤쪽으로 보낸다. (교재 116p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b22e5",
   "metadata": {},
   "source": [
    "$ x_1 -> w_1x_1 = p_1  -> p_1이 시그모이드를 만나 y = \\frac{1}{a + e^{-p_1}} -> w_2y = p_2 -> p_2가 시그모이드를 만나 z = \\frac{1}{1 + e^{-p_2}} 의 순서대로 진행되며 $\n",
    "\n",
    "$ z(= 0 ~1)를 이용하여 L(LossFunction) = -\\frac{1}{2}(d - z)^2 을 구할 수 있다. $\n",
    "\n",
    "* $ d  = 0 ~ 1 사이의 값 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd925aca",
   "metadata": {},
   "source": [
    "$$ w_1^{(n+1)} = w_1^{(n)} - \\gamma\\frac{dL(w^{(n)})}{dw_1} $$\n",
    "\n",
    "$$ w_2^{(m+1)} = w_2^{(m)} - \\gamma\\frac{dL(w^{(m)})}{dw_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3433d5d",
   "metadata": {},
   "source": [
    "% 가중치($w$)와 바이어스($b$)를 구하기 위해서는 경사하강법을 사용한다.\n",
    "\n",
    "- $ w_2 $ 구하기 (경사 하강법을 활용)\n",
    "\n",
    "$$ \\frac{dL}{dw_2} = \\frac{dL}{dz} * \\frac{z}{dp_2} * \\frac{p_2}{dw_2} = (d - z) * z(1 - z) * y $$\n",
    "\n",
    "- $ w_1 $ 구하기\n",
    "\n",
    "$$ \\frac{dL}{dw_1} = \\frac{dL}{dz} * \\frac{dz}{dp_2} * \\frac{dp_2}{dy} * \\frac{dy}{dp_1} * \\frac{dp_1}{dw1} = (d - z) * z(1 - z) * w_2 * y(y - y) * x_1 $$\n",
    "\n",
    "위의 식에서 $ p_2 $를 $ w_2 $로 미분하고, 아래 식에서 $ p_2 $를 $ y $로 미분한 이유는 주어진 식($ \\frac{dL}{dw_2}, \\frac{dL}{dw_1} $)을 만족 시키기 위함이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23e7b1",
   "metadata": {},
   "source": [
    "다만, 시그모이드 함수를 계속해서 미분하다보면(데이터의 양이 늘어난다면) 1보다 작은 수가 계속 곱해져 0에 가까워진다.\n",
    "\n",
    "즉, 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워진다.\n",
    "\n",
    "-> 이런 문제를 해결하기 위한 모델 \n",
    "1. 하이퍼블릭 탄젠트(tant) : 범위를 -1 ~ 1로 잡는다. 하지만 여전히 0보다 작은 값이 있어 기울기 소실이 발생.\n",
    "2. 렐루(ReLU) : 0보다 작을 때는 무조건 0, 0보다 큰 수는 x의 값을 그대로 사용.\n",
    "3. 소프트 플러스(softplus) : 렐루(ReLU)의 변형 함수. 0보다 작은 값에 -를 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1302e",
   "metadata": {},
   "source": [
    "# [확률적 경사 하강법(Stochastic Gradient Descent, SGD)](https://twinw.tistory.com/247)\n",
    "\n",
    "경사 하강법은 불필요하게 많은 계산량은 속도를 느리게 할 뿐만 아니라, 최적 해를 찾기 전에 최적화 과정을 멈출 수도 있다.\n",
    "\n",
    "이러한 문제를 해결하는 것이 확률적 경사 하강법으로 전체 데이터가 아닌 <span style = 'color:red'>랜덤하게 추출한 일부 데이터를 사용</span>.\n",
    "\n",
    "중간 결과의 진폭이 크고 불안정해 보일 수도 있지만, <span style = 'color:red'>속도가 확연히 빠르면서도 최적 해에 근사한 값을 찾아낸다</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5270bd",
   "metadata": {},
   "source": [
    "# [모멘텀(momentum)](https://twinw.tistory.com/247)\n",
    "\n",
    "경사 하강법에 탄성을 추가.\n",
    "\n",
    "오차를 수정하기 바로 전에 <span style = 'color:red'>앞 수정 값과 방향(+, -)을 참고</span>하여 같은 방향으로 일정한 비율만 수정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf85c17",
   "metadata": {},
   "source": [
    "# [아담(Adam)](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tinz6461&logNo=221589073944)\n",
    "\n",
    "모멘텀과 알엠에스트롭 방법을 합친 결과."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c157248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_5728/2290190745.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kwang\\AppData\\Local\\Temp/ipykernel_5728/2290190745.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    keras.optimizer.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e - 08, decay = 0.0)\u001b[0m\n\u001b[1;37m                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "keras.optimizer.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e - 08, decay = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ea2c9",
   "metadata": {},
   "source": [
    "# 교차 엔트로피(Cross entropy)\n",
    "\n",
    "- Entropy\n",
    "$$ H(x) = -\\sum_{i=1}^n{p(x_i)logp(x_i)} $$\n",
    "\n",
    "- Cross entropy\n",
    "$$ H(x) = -\\sum_{i=1}^n{q(x_i)logp(x_i)} $$\n",
    "\n",
    "- Kinary cross entropy\n",
    "$$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m{[y_ilog(h_\\theta(x_i)) + (1 - y_i)log(1 - h_\\theta(x_i))]} $$\n",
    "$$ where, h_\\theta(x_i) = \\frac{1}{1 + e^{-\\theta_x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44efb56",
   "metadata": {},
   "source": [
    "# 오차 함수\n",
    "\n",
    "- mean squared error\n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n{(y_i - y_i^h)^2} $$\n",
    "\n",
    "- mean absolute error\n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n{|y_i - y_i^h|} $$\n",
    "\n",
    "- mean absolute percentage error\n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n{\\frac{|y_i - y_i^h|}{|y_i|}} $$\n",
    "\n",
    "- mean squared logarithmic error\n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n{(log(y_i + 1) - log(y_i^h + 1))^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15e9ab",
   "metadata": {},
   "source": [
    "# 소프트맥스(Softmax)\n",
    "\n",
    "<span style = 'color:red'>총합이 1인 형태</span>로 바꿔서 계산해주는 함수.\n",
    "\n",
    "큰 값이 두드러지게 나타나고 작은 값은 더 작아진다.\n",
    "\n",
    "이 값이 <span style = 'color:red'>교차 엔트로피</span>를 지나 [1., 0., 0.]으로 변하게 되면 우리가 원하는 <span style = 'color:red'>원-핫 인코딩 값</span>, 즉 <span style = 'color:red'>하나만 1이고 나머지는 모두 0인 형태</span>로 전환시킬 수 있다.\n",
    "\n",
    "$$ p_i = \\frac{e^{zi}}{\\sum_{j=1}^ke^{zj}}, i = 1, 2, 3, ..., k $$ \n",
    "\n",
    "- k개의 클래스\n",
    "- i 번째 클래스\n",
    "- $p_i$, i 번째가 정답일 확률\n",
    "\n",
    "ex)\n",
    "\n",
    "$ softmax(z) = [\\frac{e^{z_1}}{\\sum_{j=1}^3{e^{zj}}}, \\frac{e^{z_2}}{\\sum_{j=1}^3{e^{zj}}}, \\frac{e^{z_3}}{\\sum_{j=1}^3{e^{zj}}}] = [p_1, p_2, p_3] $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d3127e",
   "metadata": {},
   "source": [
    "# 모델의 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af79648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a9feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "data_set = np.loadtxt('../교재 코드/deeplearning-for-everyone-2nd-master/dataset/ThoraricSurgery.CSV',\n",
    "                     delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8a5f918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[293.     1.     3.8  ...   0.    62.     0.  ]\n",
      " [  1.     2.     2.88 ...   0.    60.     0.  ]\n",
      " [  8.     2.     3.19 ...   0.    66.     1.  ]\n",
      " ...\n",
      " [406.     6.     5.36 ...   0.    62.     0.  ]\n",
      " [ 25.     8.     4.32 ...   0.    58.     1.  ]\n",
      " [447.     8.     5.2  ...   0.    49.     0.  ]]\n",
      "\n",
      "(470, 18)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_set); print()\n",
    "\n",
    "print(data_set.shape);print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5246ee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 0.1495 - accuracy: 0.8468\n",
      "Epoch 2/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.8511\n",
      "Epoch 3/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.8489\n",
      "Epoch 4/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
      "Epoch 5/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
      "Epoch 6/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
      "Epoch 7/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.8511\n",
      "Epoch 8/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.8511\n",
      "Epoch 9/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.8511\n",
      "Epoch 10/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.8532\n",
      "Epoch 11/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.8532\n",
      "Epoch 12/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.8489\n",
      "Epoch 13/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.8532\n",
      "Epoch 14/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.8489\n",
      "Epoch 15/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.8489\n",
      "Epoch 16/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.8511\n",
      "Epoch 17/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.8511\n",
      "Epoch 18/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.8511\n",
      "Epoch 19/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.8489\n",
      "Epoch 20/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.8532\n",
      "Epoch 21/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.8511\n",
      "Epoch 22/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.8511\n",
      "Epoch 23/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.8489\n",
      "Epoch 24/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.8447\n",
      "Epoch 25/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.8511\n",
      "Epoch 26/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.8511\n",
      "Epoch 27/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.8511\n",
      "Epoch 28/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.8511\n",
      "Epoch 29/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.8574\n",
      "Epoch 30/100\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.8511\n",
      "Epoch 31/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.8511\n",
      "Epoch 32/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.8511\n",
      "Epoch 33/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.8489\n",
      "Epoch 34/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.8553\n",
      "Epoch 35/100\n",
      "47/47 [==============================] - 0s 929us/step - loss: 0.1450 - accuracy: 0.8404\n",
      "Epoch 36/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.8511\n",
      "Epoch 37/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
      "Epoch 38/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
      "Epoch 39/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
      "Epoch 40/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.8511\n",
      "Epoch 41/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.8511\n",
      "Epoch 42/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.8511\n",
      "Epoch 43/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.8511\n",
      "Epoch 44/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.8511\n",
      "Epoch 45/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.8511\n",
      "Epoch 46/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.8489\n",
      "Epoch 47/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.8447\n",
      "Epoch 48/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.8511\n",
      "Epoch 49/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.8511\n",
      "Epoch 50/100\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.8532\n",
      "Epoch 51/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.8489\n",
      "Epoch 52/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.8532\n",
      "Epoch 53/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.8532\n",
      "Epoch 54/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.8468\n",
      "Epoch 55/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.8532\n",
      "Epoch 56/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.8532\n",
      "Epoch 57/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.8553\n",
      "Epoch 58/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.8553\n",
      "Epoch 59/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.8532\n",
      "Epoch 60/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.8511\n",
      "Epoch 61/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.8617\n",
      "Epoch 62/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.8468\n",
      "Epoch 63/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.8532\n",
      "Epoch 64/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.8447\n",
      "Epoch 65/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.8532\n",
      "Epoch 66/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.8553\n",
      "Epoch 67/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.8532\n",
      "Epoch 68/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.8574\n",
      "Epoch 69/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.8532\n",
      "Epoch 70/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.8596\n",
      "Epoch 71/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.8532\n",
      "Epoch 72/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.8702\n",
      "Epoch 73/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.8574\n",
      "Epoch 74/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.8617\n",
      "Epoch 75/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.8617\n",
      "Epoch 76/100\n",
      "47/47 [==============================] - 0s 992us/step - loss: 0.1308 - accuracy: 0.8596\n",
      "Epoch 77/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.8660\n",
      "Epoch 78/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.8383\n",
      "Epoch 79/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.8511\n",
      "Epoch 80/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.8574\n",
      "Epoch 81/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.8489\n",
      "Epoch 82/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.8553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.8553\n",
      "Epoch 84/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.8447\n",
      "Epoch 85/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.8511\n",
      "Epoch 86/100\n",
      "47/47 [==============================] - 0s 998us/step - loss: 0.1444 - accuracy: 0.8447\n",
      "Epoch 87/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.8617\n",
      "Epoch 88/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.8553\n",
      "Epoch 89/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.8617\n",
      "Epoch 90/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.8532\n",
      "Epoch 91/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.8553\n",
      "Epoch 92/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.8574\n",
      "Epoch 93/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.8553\n",
      "Epoch 94/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.8553\n",
      "Epoch 95/100\n",
      "47/47 [==============================] - 0s 977us/step - loss: 0.1298 - accuracy: 0.8574\n",
      "Epoch 96/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.8617\n",
      "Epoch 97/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.8532\n",
      "Epoch 98/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.8596\n",
      "Epoch 99/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.8553\n",
      "Epoch 100/100\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.8681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d638dc7fa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_set[:, 0:17]\n",
    "y = data_set[:, 17]\n",
    "\n",
    "model = Sequential()                                            # 딥러닝의 구조를 짜고 층을 설정\n",
    "model.add(Dense(30, input_dim=17, activation='relu'))               # 딥러닝의 구조를 relu로 설정 (model.add() 함수로 새로운 층 형성)\n",
    "model.add(Dense(1, activation='sigmoid'))                           # 딥러닝의 구조는 sigmoid로 설정\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',      # 딥러닝 실행\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b86a55",
   "metadata": {},
   "source": [
    "1. model = Sequential()\n",
    "\n",
    "- 각 층은 Dense 함수를 통해 구체적으로 구조를 결정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a86d1e",
   "metadata": {},
   "source": [
    "2. Dense(30, input_dim=17, adctivation='relu') 에서\n",
    "\n",
    "- 30은 30개의 노드를 만들겠다는 뜻.\n",
    "- input_dim이라는 변수는 입력 데이터에서 몇 개의 값을 가져올지를 정함.\n",
    "- activation에서 활성 함수를 정함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9777e3c",
   "metadata": {},
   "source": [
    "3. 두 번째 Dense(1, activation='sigmoid') 에서\n",
    "\n",
    "- 첫 번째 Dense의 형성이 끝나고 out_put을 만듦.\n",
    "- out_put에서는 activation으로 sigmoid 활성 함수를 선정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88fc178",
   "metadata": {},
   "source": [
    "4. model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "- 앞서 지정한 모델이 효과적으로 구현될 수 있게 여러 가지 환경을 설정해 주면서 컴파일하는 부분.\n",
    "- 어떤 <span style = 'color:red'>오차</span>를 사용할지 지정. (mean_squared_error : 평균 제곱 오차)\n",
    "- metrics() 함수는 모델이 컴파일될 때 모델 수행 결과를 나타내게끔 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e3689",
   "metadata": {},
   "source": [
    "5. model.fit(x, y, epochs=100, batch_size=10)\n",
    "\n",
    "- 앞서 컴파일 단계에서 정해진 환경을 주어진 데이터를 불러 실행시킬 때 사용되는 함수.\n",
    "- epochs=100은 각 샘플이 처음부터 끝가지 100번 재사용될 때까지 실행을 반복.\n",
    "- batch_size=10는 전체 470개의 샘플을 10개씩 끊어 넣으라는 뜻.(batch_size가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결과값이 불안정해질 수 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0f71e",
   "metadata": {},
   "source": [
    "# [Confusion Marix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "- $ Accuracy(정확도) = \\frac{TP + TN}{N} $\n",
    "\n",
    "- $ Recall = \\frac{TP}{TP + FN} $ \n",
    "\n",
    "- $ Precision = \\frac{TP}{TP + FP} $\n",
    "\n",
    "- $ Specificity(특이도) = \\frac{TN}{FP + TN} $\n",
    "\n",
    "여기서, TP : 그렇다의 정답(+, +), FN : 그렇다의 오답(+, -), FP : 그렇지 않다의 정답(-, +), TN : 그렇지 않다의 오답(-, -)\n",
    "\n",
    "=> () 안의 내용은 실제값, 예측값의 순서로 +일 경우는 긍정, -는 부정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53dca1",
   "metadata": {},
   "source": [
    "# [F-score](https://en.wikipedia.org/wiki/F-score)\n",
    "\n",
    "$$ F_1 = \\frac{2}{recall^{-1} + precision^{-1}} = 2\\frac{precision * recall}{precision + recall} = \\frac{tp}{tp + \\frac{1}{2}{(fp + fn)}} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
